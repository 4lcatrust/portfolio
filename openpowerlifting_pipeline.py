# -*- coding: utf-8 -*-
"""OpenPowerlifting Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eW67YZBtiOLJi69OqTfujJX6xnGeJLu
"""

import json
import polars as pl
import os
from google.oauth2 import service_account
from googleapiclient.discovery import build
import kagglehub

# Download Kaggle OpenPowerlifting dataset and read with Polars
path = kagglehub.dataset_download("open-powerlifting/powerlifting-database")
filename = 'openpowerlifting-2024-01-06-4c732975.csv'
filepath = f'{path}/{filename}'

# Read dataset
df = pl.read_csv(path+'/openpowerlifting-2024-01-06-4c732975.csv', infer_schema_length = 1000)

# Check shape of dataset
print(f'Dataset Row Count: {df.shape[0]:,}')
print(f'Dataset Column Count: {df.shape[1]:,}')

# Preview dataset
display(df.head())

# Checking on dataset's schema to enhance data type
display(dict(df.schema))

# Checking on dataset's null values on main column, in this case, name of the athletes
print('Is there any NULL values in `Name` column?: ' + str(df['Name'].is_empty()))

# Casting `Age` from float into int data type
df = df.with_columns(pl.col('Age').cast(pl.Int64, strict=False))

# Checking if the transformed schema are succesfully transformed or not
display(dict(df[['Age']].schema))

# Create ingestion into spreadsheet
from google.colab import userdata

# Use Google service account credentials for with enabled Spreadsheet API
SERVICE_ACCOUNT = json.loads(userdata.get('4LCA_SS_API_KEY'))
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
creds = service_account.Credentials.from_service_account_info(SERVICE_ACCOUNT, scopes=SCOPES)

# Input ID and range of a targeted spreadsheet
SPREADSHEET_ID = userdata.get('SPREADSHEET_ID')
SPREADSHEET_RANGE = 'OpenPowerlifting'
service = build('sheets', 'v4', credentials=creds)

# Insert spreadsheet count cells limitation
spreadsheet_limit_cells = round(10000000 * 0.75)
dataset_maximum_row = round(spreadsheet_limit_cells / df.shape[1])

# Create batching process
batch_size = round(dataset_maximum_row / 5)
iter = 0
while iter <= dataset_maximum_row:
    print(f'Iteration {iter} starting...')
    match iter:
        case 0:
            # For first batch, insert column name for header
            data = df[iter : iter + batch_size].rows()
            data.insert(0, df.columns)
            body = {
                'values': data
            }
            result = service.spreadsheets().values().update(
                spreadsheetId = SPREADSHEET_ID,
                range = SPREADSHEET_RANGE,
                valueInputOption = 'RAW',
                body = body
                ).execute()
            print(f"Header appended. {result['updatedCells']} cells updated.")
        case _:
            # For the rest of the batch only append the spreadsheet
            data = df[iter : iter + batch_size].rows()
            body = {
                'values': data
            }

            result = service.spreadsheets().values().append(
                spreadsheetId = SPREADSHEET_ID,
                range = SPREADSHEET_RANGE,
                valueInputOption = 'RAW',
                insertDataOption = 'INSERT_ROWS',
                body = body
                ).execute()
            print(f"Batch appended. {result['updates']['updatedCells']} cells updated.")
    iter += batch_size

print(f'Data ingestion done\n')

# Check data ingestion result
request = service.spreadsheets().get(spreadsheetId = SPREADSHEET_ID, fields = 'sheets(properties(gridProperties(rowCount,columnCount)))')
response = request.execute()
for sheet in response['sheets']:
    row_count = sheet['properties']['gridProperties']['rowCount']
    column_count = sheet['properties']['gridProperties']['columnCount']
    print(f'Row Count: {row_count} ingested')
    print(f'Column Count: {column_count} ingested')